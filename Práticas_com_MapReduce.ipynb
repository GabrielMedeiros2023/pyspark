{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "k-l6wcLAtRXU",
        "ABmHF_aMtgHu",
        "B28n0o0RHyp1",
        "667ZpPr9HKUw",
        "7eDY3XrpI5M7",
        "B0H2eWQXJAb9",
        "a75-8jsTNLlU",
        "fgNcqXABNFO2",
        "cBJbVOibO_u2",
        "IqYz6TmIyrgA",
        "gJmZp2sz1wo-",
        "Yt8iSxGx3eGn",
        "f4_32WGU39q4",
        "x5AqwPAN4apR",
        "PUJ2upE-5IZN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***PRÁTICA 01:***"
      ],
      "metadata": {
        "id": "-8Eq7GWpnfXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalando PySpark:**"
      ],
      "metadata": {
        "id": "O542Az0kno2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j ## Código para instalação da versão mais recente. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eGCR8ORn7HX",
        "outputId": "5ddfe681-19db-42cf-a844-41339047957c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalando findspark**"
      ],
      "metadata": {
        "id": "k-l6wcLAtRXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "QYKtWSRSo6xY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Iniciando o FindSpark para utilizar o PySpark depois**"
      ],
      "metadata": {
        "id": "ABmHF_aMtgHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "oo2XpL6voz76"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importando o pacote Numpy para criação de vetor**"
      ],
      "metadata": {
        "id": "B28n0o0RHyp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "0ilUELqMH4ft"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instanciando uma sessão para utilizar o MapReduce**"
      ],
      "metadata": {
        "id": "667ZpPr9HKUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "SiEUVZOgGTMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrando com os dados do vetor, conforme o código:**"
      ],
      "metadata": {
        "id": "7eDY3XrpI5M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50]) \n"
      ],
      "metadata": {
        "id": "5T4qBbRfIdKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Criando um RDD por meio de um SparkContext, usando o seguinte código:**"
      ],
      "metadata": {
        "id": "B0H2eWQXJAb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize(vetor)"
      ],
      "metadata": {
        "id": "sPfjZRQQImVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**verificação do conteúdo da variável paralelo, usando o código:**"
      ],
      "metadata": {
        "id": "E2flWm6rJf4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(paralelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHDEeKCmJfIW",
        "outputId": "36e86ae1-8896-47af-e3e1-54b10a7d23c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como o vetor já está no Spark, podemos aplicar o mapeamento usando o seguinte código:**"
      ],
      "metadata": {
        "id": "3JWBaGonJ_-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXEMPLO 1**"
      ],
      "metadata": {
        "id": "a75-8jsTNLlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x**2+x)"
      ],
      "metadata": {
        "id": "I42vQB6mKCg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, lambda x : x**2+x é uma função lambda correspondente à função matemática que queremos aplicar aos elementos da entrada de dados. O código paralelo.map faz o mapeamento da função para cada elemento da entrada."
      ],
      "metadata": {
        "id": "8jEjCAnPKE22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Colete os dados, ou seja, verifique o resultado por meio do código:\n",
        "mapa.collect()\n",
        "\n",
        "## Esse código produz a seguinte saída que queríamos:\n",
        "[110, 420, 930, 1640, 2550]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUsRuCdoKeid",
        "outputId": "78b56822-6c12-4972-ab7e-24b9847d2d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXEMPLO 2**"
      ],
      "metadata": {
        "id": "fgNcqXABNFO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Acompanhe o exemplo 2 – Listas, que tem como objetivo contar a frequência das palavras de uma lista.\n",
        "## Entre com uma lista de palavras, conforme o seguinte código:\n",
        "paralelo = spark_contexto.parallelize([\"distribuida\", \"distribuida\", \"spark\", \"rdd\", \"spark\", \"spark\"])\n",
        "\n",
        "## Com isso, a variável paralelo faz referência ao RDD.\n",
        "## Implemente uma função lambda que simplesmente associa o número 1 a uma palavra. O código fica exatamente assim:>\n",
        "funcao_lambda = lambda x:(x,1)\n",
        "\n",
        "## Ou seja, a função recebe uma variável x e vai produzir o par (x, 1).\n",
        "## Aplicando o MapReduce, visualizando o código primeiro e analisando-o em seguida. O código é dado por:\n",
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()\n",
        "\n",
        "## Observe que, na primeira linha, importamos o operador add, que será usado para somar as ocorrências das palavras mapeadas.\n",
        "\n",
        "## Já na segunda linha, realizamos muitas tarefas, como:\n",
        "\n",
        "## • O mapeamento dos dados da variável paralelo para a função lambda – ou seja, para cada palavra, criamos um par (palavra, 1).\n",
        "\n",
        "## • A aplicação da redução por meio da função reduceKey, que soma as ocorrências e as agrupa pela chave – no caso, pelas palavras.\n",
        "\n",
        "## • A coleta dos dados na etapa final, dada pela função collect, em uma lista que chamamos de mapa.\n",
        "## Para visualizar o resultado, use o seguinte código:\n",
        "for (w, c) in mapa:\n",
        "\n",
        "  print(\"{}: {}\".format(w, c))\n",
        "\n",
        "## Esse código percorre a lista mapa e imprime cada par formado pela palavra e sua respectiva ocorrência. A saída é a seguinte:\n",
        "\n",
        "##distribuida: 2\n",
        "\n",
        "##spark: 3\n",
        "\n",
        "##rdd: 1\n",
        "\n",
        "##Para concluir, feche a sessão, usando o código:\n",
        "spark_contexto.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiM30RheK0NE",
        "outputId": "547c4c58-d871-4279-8e34-0298e5ee1ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribuida: 2\n",
            "spark: 3\n",
            "rdd: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parar a aplicação:**\n"
      ],
      "metadata": {
        "id": "cBJbVOibO_u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "cKA-3CUJO6yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PRÁTICA 02:***"
      ],
      "metadata": {
        "id": "a7kIuHKBm0rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Desenvolvendo um exemplo prático de transformação e ação.*** "
      ],
      "metadata": {
        "id": "oJxnW0Tdp_B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importando e Instanciando o SparkContext:**"
      ],
      "metadata": {
        "id": "IqYz6TmIyrgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "4cZguHRGorig"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fornecendo uma lista de entrada e transformando em um RDD:**"
      ],
      "metadata": {
        "id": "gJmZp2sz1wo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1, 2, 3 ,4, 5, 3]\n",
        "lista_rdd = spark_contexto.parallelize(lista)"
      ],
      "metadata": {
        "id": "NaHMkanw15Rs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Executando uma ação de contar os elementos do RDD:**"
      ],
      "metadata": {
        "id": "Yt8iSxGx3eGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1apdNyq3lZh",
        "outputId": "37ab45b6-2a59-4f65-bebf-0114f23e963d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Criando uma função lambda que recebe um número como parâmetro e retorna um par formado pelo número do parâmetro e pelo mesmo número multiplicado por 10:**"
      ],
      "metadata": {
        "id": "f4_32WGU39q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "par_ordenado = lambda numero: (numero, numero*10)"
      ],
      "metadata": {
        "id": "BrzcjT2z4DBA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aplicando transformação flatMap com a ação collect da função lambda para a lista_rdd que vai produzir a saída [1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 3, 30]:**"
      ],
      "metadata": {
        "id": "x5AqwPAN4apR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.flatMap(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rdxuj0F4oHz",
        "outputId": "5bdfc69c-b760-4c5b-86f5-9d64483248f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 3, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aplicando transformação map com a ação collect da função lambda para a lista_rdd. Para isso, use o código a seguir, que vai produzir a saída [(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (3, 30)]:**"
      ],
      "metadata": {
        "id": "PUJ2upE-5IZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.map(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEJQTLa5MWN",
        "outputId": "2b68d4d3-318c-4239-9c40-6ab4049a82dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (3, 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fechando a sessão:**"
      ],
      "metadata": {
        "id": "C8oaOKQy5owy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "ojRGCd6_5rkW"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}